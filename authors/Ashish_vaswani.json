{
    "author": "Ashish vaswani",
    "affiliation": "Startup",
    "total_papers": 104,
    "total_citations": 160129,
    "publications": [
        {
            "title": "Attention is all you need",
            "year": "2017",
            "citations": 137241,
            "link": "No link available"
        },
        {
            "title": "Relational inductive biases, deep learning, and graph networks",
            "year": "2018",
            "citations": 3744,
            "link": "No link available"
        },
        {
            "title": "Self-attention with relative position representations",
            "year": "2018",
            "citations": 2555,
            "link": "No link available"
        },
        {
            "title": "Image transformer",
            "year": "2018",
            "citations": 1953,
            "link": "No link available"
        },
        {
            "title": "Advances in neural information processing systems",
            "year": "2017",
            "citations": 1877,
            "link": "No link available"
        },
        {
            "title": "Attention Is All You Need.(Nips), 2017",
            "year": "2017",
            "citations": 1389,
            "link": "No link available"
        },
        {
            "title": "Attention augmented convolutional networks",
            "year": "2019",
            "citations": 1314,
            "link": "No link available"
        },
        {
            "title": "Stand-alone self-attention in vision models",
            "year": "2019",
            "citations": 1304,
            "link": "No link available"
        },
        {
            "title": "Bottleneck transformers for visual recognition",
            "year": "2021",
            "citations": 1184,
            "link": "No link available"
        },
        {
            "title": "Music transformer",
            "year": "2018",
            "citations": 892,
            "link": "No link available"
        },
        {
            "title": "Tensor2tensor for neural machine translation",
            "year": "2018",
            "citations": 630,
            "link": "No link available"
        },
        {
            "title": "Efficient content-based sparse attention with routing transformers",
            "year": "2021",
            "citations": 542,
            "link": "No link available"
        },
        {
            "title": "Scaling local self-attention for parameter efficient visual backbones",
            "year": "2021",
            "citations": 442,
            "link": "No link available"
        },
        {
            "title": "Learning whom to trust with MACE",
            "year": "2013",
            "citations": 403,
            "link": "No link available"
        },
        {
            "title": "Mesh-tensorflow: Deep learning for supercomputers",
            "year": "2018",
            "citations": 401,
            "link": "No link available"
        },
        {
            "title": "One model to learn them all",
            "year": "2017",
            "citations": 394,
            "link": "No link available"
        },
        {
            "title": "Decoding with large-scale neural language models improves translation",
            "year": "2013",
            "citations": 299,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need. CoRR abs/1706.03762 (2017)",
            "year": "2017",
            "citations": 292,
            "link": "No link available"
        },
        {
            "title": "Proceedings of the 31st international conference on neural information processing systems",
            "year": "2017",
            "citations": 230,
            "link": "No link available"
        },
        {
            "title": "Relational inductive biases, deep learning, and graph networks. arXiv 2018",
            "year": "2018",
            "citations": 207,
            "link": "No link available"
        },
        {
            "title": "Stay on the path: Instruction fidelity in vision-and-language navigation",
            "year": "2019",
            "citations": 166,
            "link": "No link available"
        },
        {
            "title": "Stand-alone self-attention in vision models",
            "year": "2019",
            "citations": 162,
            "link": "No link available"
        },
        {
            "title": "Fast decoding in sequence models using discrete latent variables",
            "year": "2018",
            "citations": 136,
            "link": "No link available"
        },
        {
            "title": "Aligning context-based statistical models of language with brain activity during reading",
            "year": "2014",
            "citations": 127,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need. 2017. doi: 10.48550",
            "year": "2017",
            "citations": 123,
            "link": "No link available"
        },
        {
            "title": "Scale efficiently: Insights from pre-training and fine-tuning transformers",
            "year": "2021",
            "citations": 121,
            "link": "No link available"
        },
        {
            "title": "Decoding the neural representation of story meanings across languages",
            "year": "2017",
            "citations": 104,
            "link": "No link available"
        },
        {
            "title": "Illia Polosukhin Attention is all you need",
            "year": "2017",
            "citations": 101,
            "link": "No link available"
        },
        {
            "title": "The efficiency misnomer",
            "year": "2021",
            "citations": 100,
            "link": "No link available"
        },
        {
            "title": "Supertagging with lstms",
            "year": "2016",
            "citations": 93,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need. arXiv, 2017. doi: 10.48550",
            "year": "2017",
            "citations": 91,
            "link": "No link available"
        },
        {
            "title": "Attention Is All You Need: 31st Conference on Neural Information Processing Systems (NIPS 2017)",
            "year": "2017",
            "citations": 88,
            "link": "No link available"
        },
        {
            "title": "Theory and experiments on vector quantized autoencoders",
            "year": "2018",
            "citations": 83,
            "link": "No link available"
        },
        {
            "title": "An improved relative self-attention mechanism for transformer with application to music generation",
            "year": "2018",
            "citations": 79,
            "link": "No link available"
        },
        {
            "title": "Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention Is All You Need",
            "year": "2017",
            "citations": 79,
            "link": "No link available"
        },
        {
            "title": "DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer",
            "year": "2023",
            "citations": 73,
            "link": "No link available"
        },
        {
            "title": "Noam",
            "year": "2017",
            "citations": 73,
            "link": "No link available"
        },
        {
            "title": "Unsupervised neural hidden Markov models",
            "year": "2016",
            "citations": 73,
            "link": "No link available"
        },
        {
            "title": "JakobUszkoreit and Llion Jones,\u201c",
            "year": "2017",
            "citations": 71,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need (arXiv: 1706.03762). arXiv",
            "year": "2017",
            "citations": 69,
            "link": "No link available"
        },
        {
            "title": "Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17)",
            "year": "2017",
            "citations": 67,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need. CoRR",
            "year": "2017",
            "citations": 66,
            "link": "No link available"
        },
        {
            "title": "Attention Is All You Need. NIPS\u201917",
            "year": "2017",
            "citations": 65,
            "link": "No link available"
        },
        {
            "title": "503 \u0141ukasz Kaiser, and Illia Polosukhin",
            "year": "Unknown",
            "citations": 55,
            "link": "No link available"
        },
        {
            "title": "Simple, fast noise-contrastive estimation for large rnn vocabularies",
            "year": "2016",
            "citations": 54,
            "link": "No link available"
        },
        {
            "title": "Kaiser, L. u., & Polosukhin, I.(2017)",
            "year": "Unknown",
            "citations": 47,
            "link": "No link available"
        },
        {
            "title": "Name tagging for low-resource incident languages based on expectation-driven learning",
            "year": "2016",
            "citations": 45,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need, December 2017",
            "year": "Unknown",
            "citations": 45,
            "link": "No link available"
        },
        {
            "title": "Smaller alignment models for better translations: Unsupervised word alignment with the l0-norm",
            "year": "2012",
            "citations": 39,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2019",
            "citations": 34,
            "link": "No link available"
        },
        {
            "title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation",
            "year": "2014",
            "citations": 27,
            "link": "No link available"
        },
        {
            "title": "Attention-based image generation neural networks",
            "year": "2020",
            "citations": 26,
            "link": "No link available"
        },
        {
            "title": "Unifying bayesian inference and vector space models for improved decipherment",
            "year": "2015",
            "citations": 23,
            "link": "No link available"
        },
        {
            "title": "Rule markov models for fast tree-to-string translation",
            "year": "2011",
            "citations": 23,
            "link": "No link available"
        },
        {
            "title": "Attention is all you need In: Guyon I, Luxburg UV, Bengio S, et al, eds",
            "year": "Unknown",
            "citations": 23,
            "link": "No link available"
        },
        {
            "title": "Models and training for unsupervised preposition sense disambiguation",
            "year": "2011",
            "citations": 19,
            "link": "No link available"
        },
        {
            "title": "LlionJones",
            "year": "2017",
            "citations": 16,
            "link": "No link available"
        },
        {
            "title": "AidanN. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need",
            "year": "Unknown",
            "citations": 16,
            "link": "No link available"
        },
        {
            "title": "Deepconsensus: Gap-aware sequence transformers for sequence correction",
            "year": "2021",
            "citations": 15,
            "link": "No link available"
        },
        {
            "title": "Model invertibility regularization: Sequence alignment with or without parallel data",
            "year": "2015",
            "citations": 15,
            "link": "No link available"
        },
        {
            "title": "Fast, greedy model minimization for unsupervised tagging",
            "year": "2010",
            "citations": 14,
            "link": "No link available"
        },
        {
            "title": "Efficient structured inference for transition-based parsing with neural networks and error states",
            "year": "2016",
            "citations": 13,
            "link": "No link available"
        },
        {
            "title": "Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging",
            "year": "2010",
            "citations": 11,
            "link": "No link available"
        },
        {
            "title": "Multi-task multi-modal machine learning system",
            "year": "2020",
            "citations": 10,
            "link": "No link available"
        },
        {
            "title": "Visualizing music self-attention",
            "year": "2018",
            "citations": 8,
            "link": "No link available"
        },
        {
            "title": "Attention is your need",
            "year": "2017",
            "citations": 8,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2020",
            "citations": 7,
            "link": "No link available"
        },
        {
            "title": "Simple and Efficient ways to Improve REALM",
            "year": "2021",
            "citations": 5,
            "link": "No link available"
        },
        {
            "title": "Towards a better understanding of vector quantized autoencoders",
            "year": "2018",
            "citations": 5,
            "link": "No link available"
        },
        {
            "title": "Fast decoding in sequence models using discrete latent variables",
            "year": "2020",
            "citations": 3,
            "link": "No link available"
        },
        {
            "title": "394 \u201cAttention is all you need,\u201d",
            "year": "2017",
            "citations": 3,
            "link": "No link available"
        },
        {
            "title": "509 Polosukhin, I.(2017)",
            "year": "Unknown",
            "citations": 3,
            "link": "No link available"
        },
        {
            "title": "Fully attentional computer vision",
            "year": "2022",
            "citations": 2,
            "link": "No link available"
        },
        {
            "title": "Image transformer",
            "year": "2018",
            "citations": 2,
            "link": "No link available"
        },
        {
            "title": "Documentary Linguistics and Computational Linguistics: A response to Brooks",
            "year": "2015",
            "citations": 2,
            "link": "No link available"
        },
        {
            "title": "The international workshop on language preservation: An experiment in text collection and language technology",
            "year": "2013",
            "citations": 2,
            "link": "No link available"
        },
        {
            "title": "Multi-task multi-modal machine learning system",
            "year": "2022",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "Attention-based image generation neural networks",
            "year": "2022",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2022",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "Local self-attention computer vision neural networks",
            "year": "2021",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2021",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2021",
            "citations": 1,
            "link": "No link available"
        },
        {
            "title": "ATTENTION-BASED IMAGE GENERATION NEURAL NETWORKS",
            "year": "2024",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2024",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2024",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention-based image generation neural networks",
            "year": "2023",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Guest Editorial Introduction to the Special Section on Transformer Models in Vision",
            "year": "2023",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2023",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Oparte na uwadze sieci neuronowe z transdukcj\u0105 sekwencji",
            "year": "2023",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Sistema, meio n\u00e3o transit\u00f3rio de armazenamento em computador e m\u00e9todo de redes neurais de transdu\u00e7\u00e3o de sequ\u00eancias baseadas em aten\u00e7\u00e3o",
            "year": "2022",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer",
            "year": "2022",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers",
            "year": "2022",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention-based sequence transduction neural networks",
            "year": "2021",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
            "year": "2021",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Large Scale Multi-Domain Multi-Task Learning with MultiModel",
            "year": "2018",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Smaller, Faster And Accurate Models For Statistical Machine Translation",
            "year": "2014",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Attention Is All You Need Study Notes",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Relational inductive biases, deep learning, and graph networks (\u5173\u7cfb\u5f52\u7eb3\u504f\u5dee, \u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u5f62\u7f51\u7edc)",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Bottleneck Transformers for Visual Recognition-Supplementary",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Quoc V. Le Google Brain",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Self-Attention based Feature Extractors for 3D Object Detection in Point Clouds",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Relational inductive biases, deep learning, and graph networks (\u95dc\u4fc2\u6b78\u7d0d\u504f\u5dee, \u6df1\u5ea6\u5b78\u7fd2\u548c\u5716\u5f62\u7db2\u7d61)",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        },
        {
            "title": "Joint Word Alignment and Decipherment Improves Machine Translation",
            "year": "Unknown",
            "citations": 0,
            "link": "No link available"
        }
    ],
    "summary_of_fields": "Deep Learning"
}